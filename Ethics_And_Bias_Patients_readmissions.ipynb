{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pres264/AI-Development-Workflow-Assignment---Predicting-Patient-Readmission-Risk/blob/main/Ethics_And_Bias_Patients_readmissions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnGSQee527-7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnPTFB25PMS"
      },
      "source": [
        "# **ETHICS & BIAS**\n",
        "## ðŸŽ¯ **1. How biased training data affects patient outcomes**:\n",
        "biasness in data training often leads o unbalanced treatment in patients. In our case, a bias data might like:\n",
        "- underrepresented data of certain demographics (minority communities, different age groups, low income individuals or elder women) - This type of bias may result in fewer post-discharge interventions, increased complications or highter readmission risk thus directly harming the patient care and widening health disparities.\n",
        "\n",
        "For instace, patience from rural setups are underrepresentedin the training set, making the model learn from urban patients hence leading to skewed results.\n",
        "\n",
        "## **Strategy to Mitigate Bias**\n",
        "Use *stratified sampling and fairness-aware preprocessing techniques.*\n",
        "\n",
        "- **Stratified Sampling** during data collection ensures all patient subgroups (age, gender, ethnicity, income level) are proportionally represented.\n",
        "\n",
        "- Apply reweighing or re-sampling to balance the dataset (e.g. equalize readmission outcomes across groups).\n",
        "\n",
        "Use tools like:\n",
        "\n",
        "ðŸ§° Fairlearn or IBM AI Fairness 360 to audit and mitigate model bias.\n",
        "\n",
        "ðŸ“Š Evaluate fairness metrics like Demographic Parity or Equal Opportunity alongside accuracy.\n",
        "\n",
        "\n",
        "## âœ…**2. Trade-offs in Healthcare AI**\n",
        "\n",
        "## **trade-off between model interpretability and accuracy in healthcare.**\n",
        "interpretability is critical in healthcare compared to slight gain in accuracy, especialy when the model's decision could affect a patient's life.\n",
        "Factors considered include:\n",
        "- **Transparency** - interpretability are easy to explain to the user unlike accuracy which is usually in code format making it hard to explain.\n",
        "- **Trust & Adoptation** - Users have higher Understanding of the logic while accuracy is hard unless explained with SHAP/LIME.\n",
        "- **Perfomance** - Interpretability has lower predictive power unlike accuracy which have higher predictive accuracy.\n",
        "- **Use Case Fit** - Interpretability is Better for compliance and sensitive decisions while Accuracy is Best for high-stakes predictive tasks.\n",
        "\n",
        "## **Impact of limited computational resources in hospitals**\n",
        "Hospitals facing:\n",
        "- Low-spec computers\n",
        "- Limited GPU/CPU resources\n",
        "- Strict power or network constraints\n",
        "May need to rely on less models that consume less resources as Complex models like XGBoost or deep learning may be too resource-intensive to run in real-time.\n",
        "\n",
        "The Affected hospitals may need to:\n",
        "* Use simpler models (e.g., logistic regression) for deployment.\n",
        "* Perform heavy computation offsite via cloud APIs, and send only predictions to hospital systems.\n",
        "* Optimize models with quantization or pruning to reduce resource use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPnMHUrfPGOR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPXLmgwdKiek/391vdQwK0k",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
